---
title: "MATH501 Modelling and Analytics for Data Science Coursework"
author: "Maravegias"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output: 
  pdf_document:
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load libraries, include=FALSE, message = FALSE}
library(tidytext)
library(tidyr)
library(readr)
library(dplyr)
library(ggplot2)
library(scales)
library(grid)
library(lemon)
library(knitr)
```

```{r functions to be used, include=FALSE}
plot_gghistogram <- function(data = df_tele, 
                             x, 
                             x_lab = '',
                             hue = churn,
                             title_ = '',
                             fill_color_1 = 'black', 
                             fill_color_2 = 'black',
                             color_color_1 = 'tomato',
                             color_color_2 = 'steelblue') {
  data %>%
    ggplot()+
    geom_histogram(aes(x = x, fill = hue), alpha=0.5)+
    labs(title = title_,
        x = x_lab,
        y = "Count",
        fill = "Did user left?")+
    theme(axis.text = element_text(size = 11, color = "black"),
        axis.title = element_text(size = 11, color = "black")) +
    scale_color_manual(values = c("yes" = fill_color_1, 
                                "no" = fill_color_2)) +
    scale_fill_manual(values = c("yes" = color_color_1, 
                                "no" = color_color_2))+
    theme(plot.title = element_text(size=12))
}

plot_boxplot <- function(data = df_tele, 
                         y, 
                         hue = churn,
                         x_lab = '',
                         y_lab = '',
                         title_ = '',
                         fill_color_1 = 'black', 
                         fill_color_2 = 'black',
                         color_color_1 = 'tomato',
                         color_color_2 = 'steelblue') {
  data %>%
    ggplot(aes(hue, y, fill=hue))+
    geom_boxplot(varwidth = TRUE)+
    labs(title = title_,
      x=x_lab,
         y=y_lab,
    fill = "Did user left?")+
    theme(axis.text = element_text(size = 11, color = "black"),
        axis.title = element_text(size = 11, color = "black")) +
    scale_color_manual(values = c("yes" = fill_color_1, 
                                "no" = fill_color_2)) +
    scale_fill_manual(values = c("yes" = color_color_1, 
                                "no" = color_color_2))+
    theme(plot.title = element_text(size=12))+
  stat_summary(fun = mean,
               colour="darkblue",
               geom = "point",
               shape = 18,
               size = 3,
               show.legend = FALSE) +
  stat_summary(fun = mean,
               colour = "darkblue",
               geom = "text",
               show.legend = FALSE,
               vjust = -0.7,
               aes(label = round(..y.., digits = 2))) +
  theme(legend.position = "bottom")
}

plot_scatter <- function(data = df_tele, 
                         x,
                         y,
                         x_lab = '',
                         y_lab = '',
                         title_ = '',
                         hue = churn, 
                         color_1 = 'tomato', 
                         color_2 = 'steelblue'){
 
  data %>%
    ggplot()+
    geom_point(aes(x = x, y = y, color = hue))+
    labs(title = title_,
      x = x_lab,
         y = y_lab,
         color = "Did user left?")+
    theme(axis.text = element_text(size = 11, color = "black"),
          axis.title = element_text(size = 11, color = "black"))+
    scale_color_manual(values = c("yes" = color_1, 
                            "no" = color_2))
}

rounding_two_decimals_percentage<- function(x){
  round(x*100, 2)
}
```
# Machine Learning Task

This report aims to construct a classification prediction model, in order to predict
if a customer is going to stay or switch to a different telecommunication provider.

## Machine Learning Part (a)
### Data exploration
```{r load file, message=FALSE, include=FALSE}
setwd("C:/Users/kwsta/master_projects/math501")
df_tele <- read.table("churndata.txt")
df_tele$churn <- as.factor(df_tele$churn) # converting churn to a factor variables
d <- dim(df_tele)
attach(df_tele)
```
The data set has information about 500 costumers of a telecom company. It consists 
of `r d[1]` rows and `r d[2]` columns.
```{r show_df_head, echo=FALSE}
kable(head(df_tele),
      caption = 'Head of the data set')
```
The tables below gives us statistical information regarding the variables of our dataset.
```{r summary_stats, echo=FALSE, warning=FALSE, message=FALSE}
df_tele %>%
  group_by(churn) %>%
  summarise(mean_upload = mean(upload),
            mean_webget = mean(webget),
            mean_callwait = mean(callwait)) %>%
  kable(caption = 'Mean values of the variables (group by churn)')

kable(summary(df_tele),
      caption = 'Summary Statistics')
```
From **table 2** above we can notice that there is a difference in the means of the
customers who switched to another telecommunication provider.

Furthermore it is important to say, that we have an unbalanced data set, since 
the customers who switched to a different provider account for only the `r rounding_two_decimals_percentage(103/500)`% of our sample.
An unbalanced data set can affect our model's performance. Models trained on unbalanced 
datasets usually suffer from poor accuracy when have to generalize, since the 
algorithm has fewer examples of the minority class, thus will be more biased towards the majority class. 

```{r initializing_the_plots, echo=FALSE}

upload_hist <- plot_gghistogram(x=upload, x_lab='Upload speed', title_ = 'Upload speed distribution')
webget_hist <- plot_gghistogram(x=webget, x_lab='Mean time to load a webpage', title_ ='Webpage mean loading time\ndistribution')
enqcount_hist <- plot_gghistogram(x=enqcount, x_lab='Times called', title_ = 'Enquiry count distribution')
callwait_hist <- plot_gghistogram(x=callwait, x_lab='Waiting time', title_ ='Waiting time to be attended\nby customer service distribution')

upload_box <- plot_boxplot(y=upload, y_lab = 'Upload speed')
webget_box <- plot_boxplot(y=webget, y_lab='Mean time to load')
enqcount_box <- plot_boxplot(y=enqcount, y_lab='Times called')
callwait_box <- plot_boxplot(y=callwait, y_lab='Waiting time')
```
### Data Visualization

**Histograms**

```{r ploting_hists, echo=FALSE, message=FALSE }
grid_arrange_shared_legend(upload_hist, webget_hist, enqcount_hist, callwait_hist, nrow = 2, ncol = 2)
```


While observing the histogramms we can conclude the following:

1. The majority of the customers, have not switched to another provider. 
2. The proportion of customers who have switched, tend to have higher upload speed.
3. The mean time to load a webpage, for he majority of the customers who remained 
loyal the company, is between 200 - 400. If the value increases, the chance for 
a customer to switch provider increases.
4. The customers who have called the company 7 times tend to switch provider.
5. The distribution of waiting time for the both classes is similar, however for
values above 12.5 there are more customers who switched provider.

**Boxplots**
```{r plotting_box, echo=FALSE, message=FALSE}
grid_arrange_shared_legend(upload_box, webget_box, enqcount_box, callwait_box, nrow = 2, ncol = 2)
detach(df_tele)
```


From the boxplots we notice the below:

1. The boxplot of uploading speed for those who have not switched, is relatively narrow,
indicating that overall there is a small degree of variation of their upload speeds. 
Those who have switched provider, have a higher variance of upload speed. 
Furthermore it is clear that those who have switched tend to have higher upload speeds, 
while their mean value is about 2.5 points higher, and the max observed value is 
about 3 points greater from the max speed of the other group. Also the customers 
falling within the IQR for the 'no' class, indicates that 50% of them have speed 
between about 7.5 and 12.5, while for the 'yes' class from about 11 (which is 
almost the media of the 'no' class) to above 15. For both classes upload speeds 
below 5 have been reported.

2. We notice that the boxplot indicating the mean time to load a webpage is too 
narrow (whiskers included), for the group who stayed in the company, indicating 
a very small variance. In more details the 75% of this group witness mean waiting 
time about 180-400. Their appear to be a few outlines at the value 1000. 
On the other hand we notice, that the loading time varies more for those who 
switched providers, since 50% of them experience loading time from about 380 to 700. 
The upper quartile of the 'no' class ends almost where the lower quartile of the 'yes' class starts. 
We can clearly state that the group who switched provider have greater waiting 
times, as the greater median, mean and previous mentioned analysis indicates.

3. Those who have not switched provider, have contracted less times the company 
in average (3.13), compare to those who have (4.36). This can be explained, because 
customers facing connection issues tend to call more, and if their issues are not 
being solved they tend to switch providers. Also we notice that the max times someone 
have called the company is 7 times. Furthermore the boxplot for the group who switched
provider have no upper whisker, since the limit of the IQR is the same of the upper limit of the values. 
This can be explained by the extreme skewness of the data (the majority of the values are 7).
Finally we notice that there are some customers in both classes who have never called, 
while there are significant less from 'no' class who have called more than 5 times 
compared to the 'yes'. The IQR range of the 'no' class is between 2 - 5 calls, 
while for the 'yes' from 2-7. In general we can argue that those who switched provider 
tend to call more times.

5. There appears not to be any great difference of the waiting time between the 
two groups, while the median seems the same (8). The IQR of those who switched provider, 
is little longer (from 7.5 to 11), and the whiskers as well, indicating higher variation. 
The max value for those who switched is about 15, while for the other group 12.5. 
Both groups show some outlines for the value of 0, indicating that many customers did 
not waited (maybe because they never called) to be attended by a customer service operator.
In general this feauture appears not to be a signficant factor, however there is a little 
evidence that those who switched provider tend to have experience longer waiting times.

## Machine Learning Part (b)

```{r train_test_split, echo=FALSE}
set.seed(1)
df_subset <- sample(500, 350)

df_train_tele <- df_tele[df_subset,]
df_test_tele <- df_tele[-df_subset,]
```
In order to produce a classification model, we firstly randomly split our data set into a 
training set, containing 350 randomly chosen data points, and a test set containing 
the remaining 150 data points.

## Machine Learning Part (c)
Firstly we are going to use the $k-Nearest Neighbors (KNN)$ classifier to make predictions.

The KNN classifier makes predictions based on the distance (Euclidean distance in our case), thus the scale
of the variables matters. If we do not scale the data, variables with high variance (wider range),
will effect more the Euclidean distance and will appear to be more important for
the determination the target value.
Since we do not know the scales of our variables, its wise to scale our data.
However, since our variables always get positive numbers as values, we used the 
max-min normalization. We have to keep in mind normalization does not
handle the outlines well, but according to our previous analysis there are not many.

We will use the formula below to normalize each value of our variables (except of the target variable 'churn').
$$
x' =\frac{x - min(x)}{max(x)-min(x)}
$$
Where:

 * $x'$: the normalized value
 * $x$: the original value

```{r min_max_normalizer, include=FALSE}
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

df_train_tele_norm <- normalize(df_train_tele[,-5])
df_train_tele_norm$churn <- df_train_tele$churn

df_test_tele_norm <- normalize(df_test_tele[,-5])
df_test_tele_norm$churn <- df_test_tele$churn
```

Firstly we are going to use the plain **knn()** function to make predictions, and
then compare it with the leave-one-out cross-validation method.

```{r vanilla_knn, echo=FALSE}
library(class)
knn_1 <- knn(train = df_train_tele_norm[,-5], 
                      test = df_test_tele_norm[,-5], 
                      cl=df_train_tele_norm$churn, 
                      k=1)
knn_1_accuracy <- mean(knn_1 == df_test_tele_norm$churn)
knn_1_error <- mean(knn_1 != df_test_tele_norm$churn)

knn_1_conf_matrix <- table(knn_1, df_test_tele_norm$churn)

rownames(knn_1_conf_matrix) <- c("No","Yes")
kable(knn_1_conf_matrix,
       col.names = c('No','Yes'),
      caption = 'Confusion Matrix for k=1')
```
The test error for the KNN algorithm for $k=1$ is `r rounding_two_decimals_percentage(knn_1_error)`%.
We notice that the algorithm struggles to predict correctly those who switched provider (true positives), while the majority of 
the values are assigned as no, resulting 31 false negatives, thus a recall of
of only `r rounding_two_decimals_percentage(knn_1_conf_matrix[2,2]/(knn_1_conf_matrix[2,2]+knn_1_conf_matrix[1,2]))`%. This issue can be explained by the imbalance of our set.

### Leave-One-Out Cross-Validation
In order to find the optimal value of $k$, we are going to use the leave-one-out 
cross-validation for the training data set.

According to the graph below, for $k=1$ or $k=7$ we have the same error,
however it is important to check the confusion matrices.
```{r cross_validation, include=FALSE}
set.seed(1)
cross_knn_error <- c()
error <- c()
for(i in 1:100){
  cross_knn <- knn.cv(df_train_tele_norm[,-5], 
                      df_train_tele_norm[,5], 
                      k = i, 
                      l = 0, 
                      prob = FALSE, 
                      use.all = TRUE)
  cross_knn_error[i] <- mean(cross_knn != df_train_tele_norm[,5])
}
```

```{r plot_errors, echo=FALSE}
plot(cross_knn_error[1:30], 
     xlab = 'k', 
     ylab = '(Test) Classification Error', 
     col = 'blue',
     main = '(Test) Error Rate vs Neighbors (k)',
     type = 'b')
# at a line to show the min value
abline(h = min(cross_knn_error), col = "darkorange", lty = 5)
```


```{r knn_with_best_k, echo=FALSE}
set.seed(1)
cross_knn_7 <- knn.cv(df_train_tele_norm[,-5], 
                      df_train_tele_norm[,5], 
                      k = 7, 
                      l = 0, 
                      prob = FALSE, 
                      use.all = TRUE)
cross_knn_7_error <- mean(cross_knn_7 != df_train_tele_norm[,5])
cross_knn_7_accuracy <- mean(cross_knn_7 == df_train_tele_norm[,5])
cross_knn_7_conf_matrix <- table(cross_knn_7, df_train_tele_norm[,5])


cross_knn_1 <- knn.cv(df_train_tele_norm[,-5], 
                      df_train_tele_norm[,5], 
                      k = 1, 
                      l = 0, 
                      prob = FALSE, 
                      use.all = TRUE)
cross_knn_1_error <- mean(cross_knn_1 != df_train_tele_norm[,5])
cross_knn_1_conf_matrix <- table(cross_knn_1, df_train_tele_norm[,5])

rownames(cross_knn_7_conf_matrix) <- c("No","Yes")
kable(cross_knn_7_conf_matrix,
       col.names = c('No','Yes'),
      caption = 'Confusion Matrix for k=7')

rownames(cross_knn_1_conf_matrix) <- c("No","Yes")
kable(cross_knn_1_conf_matrix,
       col.names = c('No','Yes'),
      caption = 'Confusion Matrix for k=1')
```

By calculating using the LOOCV, for both values of $k$, the test error is `r rounding_two_decimals_percentage(cross_knn_7_error)`%.
We notice that it performs really better compared to the simple KNN case. However, despite the fact 
that for $k=1$ the model predicts better the true positives (those who switched providers) resulting a recall `r rounding_two_decimals_percentage(cross_knn_1_conf_matrix[2,2]/(cross_knn_1_conf_matrix[2,2]+cross_knn_1_conf_matrix[1,2]))`% compared to the `r rounding_two_decimals_percentage(cross_knn_7_conf_matrix[2,2]/(cross_knn_7_conf_matrix[2,2]+cross_knn_7_conf_matrix[1,2]))`%,
fewer $k$-neighbors correspond to higher model complexity (Müller & Guido, 2017), 
meaning that our model might not generalize. Thus we would prefer the value for
$k=7$.

## Machine Learning part (d)

We are going to implement the random forest (bagging) method to construct another classifier.

### Measuring the importance of the variables

The figures below shows us the importance of the variables for predicting churn 
using the random forest classifier.

```{r machine_learning_part_d, echo=FALSE, warning=FALSE, message=FALSE}
library(randomForest)
set.seed(1)
bag_tree_train <-randomForest(formula =  df_train_tele[,5] ~., 
                              data=df_train_tele[,-5], 
                              mtry=4, 
                              importance=TRUE,
                              ntree=500)
varImpPlot(bag_tree_train)
kable(importance(bag_tree_train),
      caption = "Variables' importance") 
```

 * **Mean decrease Accuracy** reflects how much accuracy is lost by excluding the respective variable,
thus the more the accuracy decrease the more important this variable is. 

 * **Mean decrease Gini** shows the total decrease in node impurity resulted by the splits
of a given variable, averaged over all trees. If a variable is useful. it tends to split a node into pure single class nodes. Thus, removing such variable, results a decrease in the average purity gained due to this variable. 

We notice that webget and enqcount results the most mean decrease accuracy and gini.
Also, upload scores higher in the mean decrease accuracy than callwait, while callwait scores better in gini, but with smaller spread.  
In overall, we could argue that webget and enqcount are the most significant variables for classifying with random forest if a customer will switch provider.

### Making predictions

```{r random_forest_acc, echo=FALSE}
bag_tree_pred <- predict(bag_tree_train, newdata = df_test_tele[,-5])
bag_tree_train_acc <- mean(bag_tree_pred == df_test_tele[,5])
bag_tree_train_error <- mean(bag_tree_pred != df_test_tele[,5])
rf_matrix <-table(bag_tree_pred, df_test_tele[,5])
kable(rf_matrix,
      caption = 'ConfMatrix for randomForest')
```

We notice that the random forest performs better compared to the KNN models. The test error for the vanilla KNN with $k=1$ is `r rounding_two_decimals_percentage(knn_1_error)`%, for the LOOCV KNN for $k=7$ is `r rounding_two_decimals_percentage(cross_knn_7_error)`
while for random forest the test error is only `r rounding_two_decimals_percentage(bag_tree_train_error)`%, and thus its accuracy is `r rounding_two_decimals_percentage(bag_tree_train_acc)`%.
We notice that the random forest performed much better predicting those who switched providers (True Positives), having a recall of `r rounding_two_decimals_percentage(rf_matrix[2,2]/(rf_matrix[2,2]+rf_matrix[1,2]))`%, compared to LOOCV KNN with recall =  47.83%.
In general both KNN models, scored very poorly, on predicting those who switched providers. That might derives from the unbalance of our dataset.

## Machine Learning Part (e)

### Principal Component Analysis (PCA)
We are going to perform Principal Component Analysis (PCA) for the four variables.
PCA is a method usually used to reduce the dimensionality of a large data set of variables (Gareth et al., 2017).

```{r part_e_PCA, echo=FALSE}
tele_pca <- princomp(df_tele[,-5], cor = TRUE)
plot(tele_pca,
     main = 'The amount of information explained by each principal component')
summary(tele_pca)
```
According to the summary table, Comp.1 accounts for the 41% of the information or variance in the data,
Comp.2 for the 31%, Comp.3 for the 18% and Comp.4 for the 8%.

```{r ploting_comp1_v_comp2, echo=FALSE}
new_variables <- tele_pca$scores[,1:2]
new_variables <- data.frame(new_variables)
new_variables$churn <- as.factor(df_tele$churn)
p<-new_variables %>%
  ggplot()+
  geom_point(aes(x=Comp.1, y = Comp.2, color=churn)) +
  labs(title = 'Comp.1 v Comp.2',
       x = "First Principal Component",
       y = "Second Principal Component") +
  coord_fixed(ratio = 1)
p
```
The information preserved in this plot equals to the proportion of the variances 
of the two components (Comp.1 and Comp.2) so is 73.5%

### First two principal components interpretetion

```{r, echo=FALSE}
biplot(tele_pca, xlim=c(-0.1,0.15), ylim=c(-0.13,0.15))
```
We notice that the **biplot** function does not produce a clean plot, to this end
for interpreting the first two principal components we implemented the **fviz_pca_biplot**,
and **fviz_pca_var** functions from the **factorextra** package, since they produce a cleaner graph.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(factoextra)
fviz_pca_var(tele_pca)
fviz_pca_biplot(tele_pca, geom = 'point')
```
From the graph above notice that following:

1. The more parallel to a principal component axis is a vector, the more it 
contributes to that principal component. Thus, the variables **upload** and 
**webget** have very high (positive) contribution on the first principal 
component (the highest contribution is due to **webget**), while the variables 
**enqcount** and **callwait** almost do not contribute at all.

2. On the other hand, the variables **enqcount** and **callwait** have very high 
(negative) contribution on the second principal component, while the variables 
**upload** and **webget** almost do not contribute at all (**webget** appear to 
be inline with the x-axis so it will no contribute at all).

3. The longer the vector, the more variability of this variable is represented by 
the two displayed principal components. The variables  **upload** and **webget** 
are longer than the **enqcount** and **callwait**, indicating that more variability 
of the former variables is presented by these two principal components, compared 
to the other two variables.

4. When two vectors form a small angle between them, it represents that the two 
variables are positively correlated. Thus, the **upload** and **webget** are 
positively correlated, as well as the  **enqcount** and **callwait**. However as 
the graph indicates **upload** and **webget** with **enqcount** and **callwait** have no correlation 
between them, since their vectors form about a 90 degrees angle. 

## Machine Learning Part (f)
```{r random_forest_with_comp1_comp2, echo=FALSE}
# splitting the data
comp_df_train <- new_variables[df_subset,]
comp_df_test <- new_variables[-df_subset,]

comp_forest <-randomForest(formula = churn ~., 
                           data=comp_df_train, 
                           mtry=2, 
                           importance=TRUE)
```

We are applying the random forest classifier (bagging) method to predict the value 
of **churn** variable based on the first two principal components.
```{r model_acc, echo=FALSE}
comp_forest_pred <- predict(comp_forest, newdata = comp_df_test[,-3])
comp_forest_acc <- mean(comp_forest_pred == comp_df_test[,3])
comp_forest_error <- mean(comp_forest_pred != comp_df_test[,3])
rf_matrix_comp <- table(comp_forest_pred, comp_df_test[,3])
kable(rf_matrix_comp,
      caption = 'Confusion matrix rf with components')
```

The test error of random forest is `r rounding_two_decimals_percentage(comp_forest_error)`%, 
which is worse than the test error obtained from the random forest which used all the variables (test error `r rounding_two_decimals_percentage(bag_tree_train_error)`%).

In general we notice that, in this case the algorithm performs poorer in predicting the True Positives and True Negatives.
However, if we take under consideration that the majority of the churn values are 'No', we notice that by this approach the model
seems to be less affected by the majority class, since has assigned less values as 'No'. It has the lowest recall for the 'No' class compared to the rest of our models (`r rounding_two_decimals_percentage(rf_matrix_comp[1,1]/(rf_matrix_comp[1,1]+rf_matrix_comp[2,1]))`%).

The first graph below is an approach to show the resulting classification rule, as a scatter plot of the two first principal components colored by the predicted class,
while the second is colored by the actual class to show a comparison.

```{r visulization_of_classification, echo=FALSE}
comp_df_test$predictions <-comp_forest_pred

comp_df_test%>%
ggplot()+
  geom_point(aes(x=Comp.1, y = Comp.2, color=predictions)) +
  labs(x = "First Principal Component",
       y = "Second Principal Component",
       title = 'Predictions') +
  coord_fixed(ratio = 1)

comp_df_test%>%
ggplot()+
  geom_point(aes(x=Comp.1, y = Comp.2, color=churn)) +
  labs(x = "First Principal Component",
       y = "Second Principal Component",
       title = 'Actual') +
  coord_fixed(ratio = 1)
```

## References

 * James, G., Witten, D., Hastie, T. and Tibshirani, R., 2017. An introduction to statistical learning. 1st ed. New York: Springer.

 * Muller, A.C. and Guido, S. 2016. Introduction to Machine Learning with Python. 1st ed. Sebastopol, CA: O'Reilly Media, Inc.