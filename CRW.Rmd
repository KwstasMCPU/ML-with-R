---
title: "MATH501 Modelling and Analytics for Data Science Coursework"
author: "Maravegias"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output: 
  pdf_document:
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries, include=FALSE, message = FALSE}
library(tidytext)
library(tidyr)
library(readr)
library(dplyr)
library(ggplot2)
library(scales)
library(grid)
library(lemon)
```

```{r functions to be used, include=FALSE}
plot_gghistogram <- function(data = df_tele, 
                             x, 
                             x_lab = '',
                             hue = churn, 
                             fill_color_1 = 'black', 
                             fill_color_2 = 'black',
                             color_color_1 = 'tomato',
                             color_color_2 = 'steelblue') {
  data %>%
    ggplot()+
    geom_histogram(aes(x = x, fill = hue), alpha=0.5)+
    labs(x = x_lab,
        y = "Count",
        fill = "Did user left?")+
    theme(axis.text = element_text(size = 13, color = "black"),
        axis.title = element_text(size = 13, color = "black")) +
    scale_color_manual(values = c("yes" = fill_color_1, 
                                "no" = fill_color_2)) +
    scale_fill_manual(values = c("yes" = color_color_1, 
                                "no" = color_color_2))
}

plot_boxplot <- function(data = df_tele, 
                         y, 
                         hue = churn,
                         x_lab = '',
                         y_lab = '',
                         fill_color_1 = 'black', 
                         fill_color_2 = 'black',
                         color_color_1 = 'tomato',
                         color_color_2 = 'steelblue') {
  data %>%
    ggplot(aes(hue, y, fill=hue))+
    geom_boxplot(varwidth = TRUE)+
    labs(x=x_lab,
         y=y_lab,
    fill = "Did user left?")+
    theme(axis.text = element_text(size = 13, color = "black"),
        axis.title = element_text(size = 13, color = "black")) +
    scale_color_manual(values = c("yes" = fill_color_1, 
                                "no" = fill_color_2)) +
    scale_fill_manual(values = c("yes" = color_color_1, 
                                "no" = color_color_2))+
  stat_summary(fun = mean,
               colour="darkblue",
               geom = "point",
               shape = 18,
               size = 3,
               show.legend = FALSE) +
  stat_summary(fun = mean,
               colour = "darkblue",
               geom = "text",
               show.legend = FALSE,
               vjust = -0.7,
               aes(label = round(..y.., digits = 2))) +
  theme(legend.position = "bottom")
}


plot_scatter <- function(data = df_tele, 
                         x,
                         y,
                         x_lab = '',
                         y_lab = '',
                         hue = churn, 
                         color_1 = 'tomato', 
                         color_2 = 'steelblue'){
 
  data %>%
    ggplot()+
    geom_point(aes(x = x, y = y, color = hue))+
    labs(x = x_lab,
         y = y_lab,
         color = "Did user left?")+
    theme(axis.text = element_text(size = 13, color = "black"),
          axis.title = element_text(size = 13, color = "black"))+
    scale_color_manual(values = c("yes" = color_1, 
                            "no" = color_2))
}



# ggplot(wages_df, aes(x = union_status, y = wage, 
#                      colour = union_status)) +
#   geom_boxplot(varwidth = TRUE) +
#   geom_jitter(width = 0.05) +
#   labs(x = "Union Status",
#        y = "Wage (Euros)") +
#   stat_summary(fun = mean, 
#                colour="darkblue", 
#                geom = "point", 
#                shape = 18, 
#                size = 3,
#                show.legend = FALSE) +
#   stat_summary(fun = mean, 
#                colour = "darkblue", 
#                geom = "text", 
#                show.legend = FALSE, 
#                vjust = -0.7, 
#                aes(label = round(..y.., digits = 2))) +
#   theme(legend.position = "bottom")


get_legend<-function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
  
# https://stackoverflow.com/questions/12539348/ggplot-separate-legend-and-plot
```
# Machine Learning Task

## Data exploration

```{r load file, message=FALSE, include=FALSE}
setwd("C:/Users/kwsta/master_projects/math501")
df_tele <- read.table("churndata.txt")
df_tele$churn <- as.factor(df_tele$churn) # converting churn to a factor variables
attach(df_tele)
```

```{r show_df_head, echo=FALSE}
head(df_tele)
```
### Summary Statistics

```{r summary_stats, echo=FALSE, warning=FALSE, message=FALSE}
df_tele %>%
  group_by(churn) %>%
  summarise(mean_upload = mean(upload),
            mean_webget = mean(webget),
            mean_callwait = mean(callwait))

summary(df_tele)
```
From the tables above we can notice that there is a difference in the means of the
customers who switched to another telecommunication provider.

## Machine Learning Part (a)

<!-- # ```{r, boxplots, echo=FALSE} -->
<!-- # par(mfrow = c(1,2)) # divide the graphics window into 2 columns -->
<!-- # boxplot(upload ~ churn, xlab = "Default", ylab = "Upload", -->
<!-- # col = c("lightblue", "orange")) -->
<!-- # boxplot(webget ~ churn, xlab = "Default", ylab = "Webget", -->
<!-- # col = c("lightblue", "orange")) -->
<!-- # par(mfrow = c(1,2)) -->
<!-- # boxplot(enqcount ~ churn, xlab = "Default", ylab = "enqcount", -->
<!-- # col = c("lightblue", "orange")) -->
<!-- # boxplot(callwait ~ churn, xlab = "Default", ylab = "callwait", -->
<!-- # col = c("lightblue", "orange")) -->
<!-- #  -->
<!-- # ``` -->

```{r initializing_the_plots, echo=FALSE}

upload_hist <- plot_gghistogram(x=upload, x_lab='Upload speed')
webget_hist <- plot_gghistogram(x=webget, x_lab='Mean time to load a webpage')
enqcount_hist <- plot_gghistogram(x=enqcount, x_lab='Enquiry count')
callwait_hist <- plot_gghistogram(x=callwait, x_lab='Waited time')

upload_box <- plot_boxplot(y=upload, y_lab = 'Upload speed')
webget_box <- plot_boxplot(y=webget, y_lab='Mean time to load a webpage')
enqcount_box <- plot_boxplot(y=enqcount, y_lab='Enquiry count')
callwait_box <- plot_boxplot(y=callwait, y_lab='Waited time')

upload_webget_scatter <- plot_scatter(x = upload, y = webget, x_lab = 'Upload speed', y_lab = 'mean load time')
upload_callwait_scatter <- plot_scatter(x = upload, y = callwait,  x_lab = 'Upload speed', y_lab = 'Wait time')
webget_callwait_scatter <- plot_scatter(x = webget, y = callwait, x_lab = 'Mean load time', y_lab = 'Wait time')
enqcount_callwait_scatter <- plot_scatter(x = enqcount, y= callwait, x_lab = 'Enquiry count', y_lab='Wait time')
enqcount_webget_scatter <- plot_scatter(x = enqcount, y= webget, x_lab = 'Enquiry count', y_lab='Mean time to load a webpage')

```

```{r ploting, echo=FALSE, message=FALSE }

grid_arrange_shared_legend(upload_hist, webget_hist, enqcount_hist, callwait_hist, nrow = 2, ncol = 2)
grid_arrange_shared_legend(upload_box, webget_box, enqcount_box, callwait_box, nrow = 2, ncol = 2)
grid_arrange_shared_legend(upload_webget_scatter, upload_callwait_scatter, webget_callwait_scatter, enqcount_callwait_scatter,nrow = 2, ncol = 2)
enqcount_webget_scatter
# https://stackoverflow.com/questions/1249548/side-by-side-plots-with-ggplot2
```

## Machine Learning Part (b)

```{r train_test_split, echo=FALSE}
set.seed(1)
df_subset <- sample(500, 350)

x_train_tele <- df_tele[df_subset,-5]
y_train_tele <- df_tele[df_subset, 5]

x_test_tele <- df_tele[-df_subset,-5]
y_test_tele <- df_tele[-df_subset, 5]

head(x_train_tele)
head(y_train_tele)
```

## Machine Learning Part (c)

KNN classifier makes predictions based on the Euclidean distance, thus the scale
of the variables matter. If we do not, variables with high variance (wider range),
will effect more the Euclidean distance and appear to be more important factor for
determining the target value.
Since we do not know the scales of our variables, its wise to scale our data.
However, since our variables always get positive numbers as values, we used the 
max-min normalization. However we have to keep in mind normalization does not
handle the outlines well.

$$
x' =\frac{x - min(x)}{max(x)-min(x)}
$$
Where:
- $x'$: the normalized value
- $x$: the original value


```{r min_max_normalizer, include=FALSE}
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

norm_x_train <- x_train_tele
  
norm_x_test <- x_test_tele 

```

```{r}
norm_x_train$upload <- normalize(norm_x_train$upload)
norm_x_train$webget <- normalize(norm_x_train$webget)
norm_x_train$callwait <- normalize(norm_x_train$callwait)
norm_x_train$enqcount <- normalize(norm_x_train$enqcount)
norm_x_test$upload <- normalize(norm_x_test$upload)
norm_x_test$webget <- normalize(norm_x_test$webget)
norm_x_test$callwait <- normalize(norm_x_test$callwait)
norm_x_test$enqcount <- normalize(norm_x_test$enqcount)
```

```{r}
library(class)
knn_tele <- function(train=norm_x_train,
                     test=norm_x_test,
                     cl=y_train_tele,
                     k=1){
  knn( train = train, test = test, cl = cl, k = k)
}
```

```{r}
knn_accuracy <- mean(knn_tele(x_train_tele, x_test_tele, y_train_tele) == y_test_tele)
knn_accuracy

norm_knn_accuracy <- mean(knn_tele(norm_x_train, norm_x_test, y_train_tele) == y_test_tele)
norm_knn_accuracy
```

```{r cross_validation}
cross_knn_error <- c()
set.seed(1)
for(i in 1:100){
  cross_knn <- knn.cv(norm_x_train, y_train_tele, k = i, l = 0, prob = FALSE, use.all = TRUE)
  cross_knn_error[i] <- mean(cross_knn != y_train_tele)
}
```

```{r}
plot(cross_knn_error[1:20], xlab = 'k', ylab = 'Error')

```

```{r knn_with_best_k}
best_knn <- knn_tele(k=5)
best_acc <- mean(best_knn == y_test_tele)
best_acc
table(best_knn, y_test_tele)
```
## Machine Learning part (d)

```{r machine_learning_part_d}
library(randomForest)
set.seed(1)
train_df <- cbind(x_train_tele, y_train_tele)
bag_tree_train <-randomForest(formula = y_train_tele ~., data=train_df, mtry=4, importance=TRUE)
varImpPlot(bag_tree_train)
```

```{r}
bag_tree_pred <- predict(bag_tree_train, newdata = x_test_tele)
bag_tree_train_acc <- mean(bag_tree_pred == y_test_tele)
bag_tree_train_acc
table(bag_tree_pred, y_test_tele)
```

```{r part_e_PCA}
norm_df_train <- cbind(norm_x_train, y_train_tele)
colnames(norm_df_train)[5] <- 'churn'
norm_df_test <- cbind(norm_x_test, y_test_tele)
colnames(norm_df_test)[5] <- 'churn'
norm_df_full <- rbind(norm_df_train, norm_df_test)
tele_pca <- princomp(norm_df_full[,-5], cor = TRUE)
summary(tele_pca)
plot(tele_pca)
comp_1 <- tele_pca$scores[,1]
comp_2 <- tele_pca$scores[,2]
```

```{r}
comp_df <- cbind(comp_1, comp_2, norm_df_full[5])
p<-comp_df %>%
  ggplot()+
  geom_point(aes(x=comp_1, y = comp_2, color=churn)) +
  labs(x = "First Principal Component",
       y = "Second Principal Component") +
  coord_fixed(ratio = 1)
p
```
The information preserved in this plot equals to the proportion of the variances 
of the two components so is 72.5%

```{r f}
comp_df_train <- comp_df[df_subset,]
comp_df_test <- comp_df[-df_subset,]

comp_forest <-randomForest(formula = churn ~., data=comp_df_train, mtry=2, importance=TRUE)
comp_forest_pred <- predict(comp_forest, newdata = comp_df_test[,-3])
comp_forest_acc <- mean(comp_forest_pred == comp_df_test[,3])
comp_forest_acc
table(comp_forest_pred, comp_df_test[,3])
```
```{r}
comp_df_test$predictions <-comp_forest_pred

comp_df_test%>%
ggplot()+
  geom_point(aes(x=comp_1, y = comp_2, color=predictions)) +
  labs(x = "First Principal Component",
       y = "Second Principal Component") +
  coord_fixed(ratio = 1)

comp_df_test%>%
ggplot()+
  geom_point(aes(x=comp_1, y = comp_2, color=churn)) +
  labs(x = "First Principal Component",
       y = "Second Principal Component") +
  coord_fixed(ratio = 1)
```