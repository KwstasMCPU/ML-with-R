---
title: "MATH501 Modelling and Analytics for Data Science Coursework"
author: "Maravegias"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output: 
  pdf_document:
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries, include=FALSE, message = FALSE}
library(tidytext)
library(tidyr)
library(readr)
library(dplyr)
library(ggplot2)
library(scales)
library(grid)
library(lemon)
```

```{r functions to be used, include=FALSE}
plot_gghistogram <- function(data = df_tele, 
                             x, 
                             x_lab = '',
                             hue = churn, 
                             fill_color_1 = 'black', 
                             fill_color_2 = 'black',
                             color_color_1 = 'tomato',
                             color_color_2 = 'steelblue') {
  data %>%
    ggplot()+
    geom_histogram(aes(x = x, fill = hue), alpha=0.5)+
    labs(x = x_lab,
        y = "Count",
        fill = "Did user left?")+
    theme(axis.text = element_text(size = 13, color = "black"),
        axis.title = element_text(size = 13, color = "black")) +
    scale_color_manual(values = c("yes" = fill_color_1, 
                                "no" = fill_color_2)) +
    scale_fill_manual(values = c("yes" = color_color_1, 
                                "no" = color_color_2))
}

plot_boxplot <- function(data = df_tele, 
                         y, 
                         hue = churn,
                         x_lab = '',
                         y_lab = '',
                         fill_color_1 = 'black', 
                         fill_color_2 = 'black',
                         color_color_1 = 'tomato',
                         color_color_2 = 'steelblue') {
  data %>%
    ggplot(aes(hue, y, fill=hue))+
    geom_boxplot(varwidth = TRUE)+
    labs(x=x_lab,
         y=y_lab,
    fill = "Did user left?")+
    theme(axis.text = element_text(size = 13, color = "black"),
        axis.title = element_text(size = 13, color = "black")) +
    scale_color_manual(values = c("yes" = fill_color_1, 
                                "no" = fill_color_2)) +
    scale_fill_manual(values = c("yes" = color_color_1, 
                                "no" = color_color_2))+
  stat_summary(fun = mean,
               colour="darkblue",
               geom = "point",
               shape = 18,
               size = 3,
               show.legend = FALSE) +
  stat_summary(fun = mean,
               colour = "darkblue",
               geom = "text",
               show.legend = FALSE,
               vjust = -0.7,
               aes(label = round(..y.., digits = 2))) +
  theme(legend.position = "bottom")
}


plot_scatter <- function(data = df_tele, 
                         x,
                         y,
                         x_lab = '',
                         y_lab = '',
                         hue = churn, 
                         color_1 = 'tomato', 
                         color_2 = 'steelblue'){
 
  data %>%
    ggplot()+
    geom_point(aes(x = x, y = y, color = hue))+
    labs(x = x_lab,
         y = y_lab,
         color = "Did user left?")+
    theme(axis.text = element_text(size = 13, color = "black"),
          axis.title = element_text(size = 13, color = "black"))+
    scale_color_manual(values = c("yes" = color_1, 
                            "no" = color_2))
}



# ggplot(wages_df, aes(x = union_status, y = wage, 
#                      colour = union_status)) +
#   geom_boxplot(varwidth = TRUE) +
#   geom_jitter(width = 0.05) +
#   labs(x = "Union Status",
#        y = "Wage (Euros)") +
#   stat_summary(fun = mean, 
#                colour="darkblue", 
#                geom = "point", 
#                shape = 18, 
#                size = 3,
#                show.legend = FALSE) +
#   stat_summary(fun = mean, 
#                colour = "darkblue", 
#                geom = "text", 
#                show.legend = FALSE, 
#                vjust = -0.7, 
#                aes(label = round(..y.., digits = 2))) +
#   theme(legend.position = "bottom")


get_legend<-function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
  
# https://stackoverflow.com/questions/12539348/ggplot-separate-legend-and-plot
```
# Machine Learning Task

## Data exploration

```{r load file, message=FALSE, include=FALSE}
setwd("C:/Users/kwsta/master_projects/math501")
df_tele <- read.table("churndata.txt")
df_tele$churn <- as.factor(df_tele$churn) # converting churn to a factor variables
attach(df_tele)
```

```{r show_df_head, echo=FALSE}
head(df_tele)
```
### Summary Statistics

```{r summary_stats, echo=FALSE, warning=FALSE, message=FALSE}
df_tele %>%
  group_by(churn) %>%
  summarise(mean_upload = mean(upload),
            mean_webget = mean(webget),
            mean_callwait = mean(callwait))

summary(df_tele)
```
From the tables above we can notice that there is a difference in the means of the
customers who switched to another telecommunication provider.

Furthermore it is important to say. that we have an imbalanced data set. Meaning 
that the customers who switched provider are way less than those who do not.
This affects our algorithm.

## Machine Learning Part (a)

<!-- # ```{r, boxplots, echo=FALSE} -->
<!-- # par(mfrow = c(1,2)) # divide the graphics window into 2 columns -->
<!-- # boxplot(upload ~ churn, xlab = "Default", ylab = "Upload", -->
<!-- # col = c("lightblue", "orange")) -->
<!-- # boxplot(webget ~ churn, xlab = "Default", ylab = "Webget", -->
<!-- # col = c("lightblue", "orange")) -->
<!-- # par(mfrow = c(1,2)) -->
<!-- # boxplot(enqcount ~ churn, xlab = "Default", ylab = "enqcount", -->
<!-- # col = c("lightblue", "orange")) -->
<!-- # boxplot(callwait ~ churn, xlab = "Default", ylab = "callwait", -->
<!-- # col = c("lightblue", "orange")) -->
<!-- #  -->
<!-- # ``` -->

```{r initializing_the_plots, echo=FALSE}

upload_hist <- plot_gghistogram(x=upload, x_lab='Upload speed')
webget_hist <- plot_gghistogram(x=webget, x_lab='Mean time to load a webpage')
enqcount_hist <- plot_gghistogram(x=enqcount, x_lab='Enquiry count')
callwait_hist <- plot_gghistogram(x=callwait, x_lab='Waited time')

upload_box <- plot_boxplot(y=upload, y_lab = 'Upload speed')
webget_box <- plot_boxplot(y=webget, y_lab='Mean time to load a webpage')
enqcount_box <- plot_boxplot(y=enqcount, y_lab='Enquiry count')
callwait_box <- plot_boxplot(y=callwait, y_lab='Waited time')

upload_webget_scatter <- plot_scatter(x = upload, y = webget, x_lab = 'Upload speed', y_lab = 'mean load time')
upload_callwait_scatter <- plot_scatter(x = upload, y = callwait,  x_lab = 'Upload speed', y_lab = 'Wait time')
webget_callwait_scatter <- plot_scatter(x = webget, y = callwait, x_lab = 'Mean load time', y_lab = 'Wait time')
enqcount_callwait_scatter <- plot_scatter(x = enqcount, y= callwait, x_lab = 'Enquiry count', y_lab='Wait time')
enqcount_webget_scatter <- plot_scatter(x = enqcount, y= webget, x_lab = 'Enquiry count', y_lab='Mean time to load a webpage')

```

```{r ploting, echo=FALSE, message=FALSE }

grid_arrange_shared_legend(upload_hist, webget_hist, enqcount_hist, callwait_hist, nrow = 2, ncol = 2)
grid_arrange_shared_legend(upload_box, webget_box, enqcount_box, callwait_box, nrow = 2, ncol = 2)
grid_arrange_shared_legend(upload_webget_scatter, upload_callwait_scatter, webget_callwait_scatter, enqcount_callwait_scatter,nrow = 2, ncol = 2)
enqcount_webget_scatter
# https://stackoverflow.com/questions/1249548/side-by-side-plots-with-ggplot2
detach(df_tele)
```

## Machine Learning Part (b)

```{r train_test_split, echo=FALSE}
set.seed(1)
df_subset <- sample(500, 350)

df_train_tele <- df_tele[df_subset,]
df_test_tele <- df_tele[-df_subset,]

head(df_train_tele)
head(df_test_tele)
```

## Machine Learning Part (c)

KNN classifier makes predictions based on the Euclidean distance, thus the scale
of the variables matter. If we do not scale the data, variables with high variance (wider range),
will effect more the Euclidean distance and will appear to be more important for
the determination the target value.
Since we do not know the scales of our variables, its wise to scale our data.
However, since our variables always get positive numbers as values, we used the 
max-min normalization. We have to keep in mind normalization does not
handle the outlines well.

$$
x' =\frac{x - min(x)}{max(x)-min(x)}
$$
Where:
 - $x'$: the normalized value
 - $x$: the original value


```{r min_max_normalizer, include=FALSE}
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

df_train_tele_norm <- normalize(df_train_tele[,-5])
df_train_tele_norm$churn <- df_train_tele$churn

df_test_tele_norm <- normalize(df_test_tele[,-5])
df_test_tele_norm$churn <- df_test_tele$churn

```


```{r cross_validation, include=FALSE}
library(class)
set.seed(1)
cross_knn_error <- c()
error <- c()
for(i in 1:100){
  cross_knn <- knn.cv(df_train_tele_norm[,-5], 
                      df_train_tele_norm[,5], 
                      k = i, 
                      l = 0, 
                      prob = FALSE, 
                      use.all = TRUE)
  cross_knn_error[i] <- mean(cross_knn != df_train_tele_norm[,5])
}
```

```{r plot_errors, echo=FALSE}
plot(cross_knn_error[1:30], xlab = 'k', ylab = 'Error')

```
According to the graph, for k=1 or k=7 we have the same error,
however it is important to check the confusion matrix.

```{r knn_with_best_k, echo=FALSE}
set.seed(1)
normalized_knn <- knn(train = df_train_tele_norm[,-5], 
                      test = df_test_tele_norm[,-5], 
                      cl=df_train_tele_norm$churn, 
                      k=1)


norm_knn_accuracy <- mean(normalized_knn == df_test_tele_norm$churn)
norm_knn_accuracy

best_knn <- knn(train = df_train_tele_norm[,-5], 
                test = df_test_tele_norm[,-5], 
                cl=df_train_tele_norm$churn, 
                k=7)

best_acc <- mean(best_knn == df_test_tele_norm$churn)
best_acc
table(normalized_knn, df_test_tele_norm$churn)
table(best_knn, df_test_tele_norm$churn)
```
We notice that for the both values of k, the algorithm struggles to predict
correct the values of yes, while the majority of the values are assigned as no.
That is an issue resulted from the imbalance of our set.

We prefer the value for k=1, since predicts better those who switched provider.

## Machine Learning part (d)
```{r machine_learning_part_d, echo=FALSE}
library(randomForest)
set.seed(1)
bag_tree_train <-randomForest(formula =  df_train_tele[,5] ~., 
                              data=df_train_tele[,-5], 
                              mtry=4, 
                              importance=TRUE,
                              ntree=500)
varImpPlot(bag_tree_train)
importance(bag_tree_train) 
```

```{r random_forest_acc, echo=FALSE}
bag_tree_pred <- predict(bag_tree_train, newdata = df_test_tele)
bag_tree_train_acc <- mean(bag_tree_pred == df_test_tele[,5])
bag_tree_train_acc
table(bag_tree_pred, df_test_tele[,5])
```

## Machine Learning Part (e)
```{r part_e_PCA, echo=FALSE}

df_tele_norm <- normalize(df_tele[,-5])
df_tele_norm$churn <- df_tele$churn

tele_pca <- princomp(df_tele_norm[,-5], cor = TRUE)
summary(tele_pca)
plot(tele_pca)
```

```{r ploting_comp1_v_comp2, echo=FALSE}
new_variables <- tele_pca$scores[,1:2]
new_variables <- data.frame(new_variables)
new_variables$churn <- as.factor(df_tele_norm$churn)
p<-new_variables %>%
  ggplot()+
  geom_point(aes(x=Comp.1, y = Comp.2, color=churn)) +
  labs(x = "First Principal Component",
       y = "Second Principal Component") +
  coord_fixed(ratio = 1)
p
```
The information preserved in this plot equals to the proportion of the variances 
of the two components so is 73.5%


## Machine Learning Part (f)
```{r random_forest_with_comp1_comp2, echo=FALSE}
# splitting the data
comp_df_train <- new_variables[df_subset,]
comp_df_test <- new_variables[-df_subset,]

comp_forest <-randomForest(formula = churn ~., 
                           data=comp_df_train, 
                           mtry=2, 
                           importance=TRUE)

varImpPlot(comp_forest)
importance(comp_forest) 
```

```{r model_acc, echo=FALSE}
comp_forest_pred <- predict(comp_forest, newdata = comp_df_test[,-3])
comp_forest_acc <- mean(comp_forest_pred == comp_df_test[,3])
comp_forest_acc
table(comp_forest_pred, comp_df_test[,3])
```

```{r visulization_of_classification, echo=FALSE}
comp_df_test$predictions <-comp_forest_pred

comp_df_test%>%
ggplot()+
  geom_point(aes(x=Comp.1, y = Comp.2, color=predictions)) +
  labs(x = "First Principal Component",
       y = "Second Principal Component",
       title = 'Predictions') +
  coord_fixed(ratio = 1)

comp_df_test%>%
ggplot()+
  geom_point(aes(x=Comp.1, y = Comp.2, color=churn)) +
  labs(x = "First Principal Component",
       y = "Second Principal Component",
       title = 'Actual') +
  coord_fixed(ratio = 1)
```


```{r}
len <- 12 
xp <- seq(-2.5, 6, length = len) 
yp <- seq(-2.5, 4, length = len) 
xygrid <- expand.grid(Comp.1 = xp, Comp.2 = yp)

col3 <- rep("lightgreen", len*len) 

for(i in 1:(len*len)){
   if(comp_forest_pred[i]== 'Yes'){ 
     col3[i] <- "blue" 
   } else {
     col3[i] <- "red"
   }
}


plot(xygrid, col = col3, main = "Tree", xlab = "pressure", ylab = "wind") 

# abline( h = c(62.5, 32.5))
# lines( x = c(985.5, 985.5), y = c(32.5, 62.5))
# 
# points(pressure, wind, col = cols, pch = 16)

```